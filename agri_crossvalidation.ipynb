{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b58b599e-2602-4f35-906a-7bd4253d8118",
   "metadata": {},
   "source": [
    "**Project Description:** A farmer reached out to you as a machine learning expert seeking help to select the best crop for his field. Due to budget constraints, the farmer explained that he could only afford to measure one out of the four essential soil measures:<br>\n",
    "•\tNitrogen content ratio in the soil <br>\n",
    "•\tPhosphorous content ratio in the soil <br>\n",
    "•\tPotassium content ratio in the soil <br>\n",
    "•\tpH value of the soil <br>\n",
    "The expert realized that this is a classic feature selection problem, where the objective is to pick the most important feature that could help predict the crop accurately. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85840481-8b39-455e-9c4c-7049a53a0057",
   "metadata": {},
   "source": [
    "**Sowing Success - How Machine Learning Helps Farmers Select the Best Crops:** <br> \n",
    "Measuring essential soil metrics such as nitrogen, phosphorous, potassium levels, and pH value is an important aspect of assessing soil condition. However, it can be an expensive and time-consuming process, which can cause farmers to prioritize which metrics to measure based on their budget constraints. Farmers have various options when it comes to deciding which crop to plant each season. Their primary objective is to maximize the yield of their crops, taking into account different factors. One crucial factor that affects crop growth is the condition of the soil in the field, which can be assessed by measuring basic elements such as nitrogen and potassium levels. Each crop has an ideal soil condition that ensures optimal growth and maximum yield. A farmer reached out to you as a machine learning expert for assistance in selecting the best crop for his field. They've provided you with a dataset called soil_measures.csv, which contains: <br>\n",
    "\"N\": Nitrogen content ratio in the soil <br>\n",
    "\"P\": Phosphorous content ratio in the soil <br>\n",
    "\"K\": Potassium content ratio in the soil <br>\n",
    "\"pH\" value of the soil <br>\n",
    "\"crop\": categorical values that contain various crops (target variable). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089aaed-c49e-4fc2-94eb-642fdbcd706d",
   "metadata": {},
   "source": [
    "Each row in this dataset represents various measures of the soil in a particular field. Based on these measurements, the crop specified in the \"crop\" column is the optimal choice for that field. In this project, you will build multi-class classification models to predict the type of \"crop\" and identify the single most importance feature for predictive performance. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a9e83-fc2d-4522-baf6-dd5df3a94410",
   "metadata": {},
   "source": [
    "Identify the single feature that has the strongest predictive performance for classifying crop types. <br>\n",
    "•\tFind the feature in the dataset that produces the best score for predicting \"crop\". <br>\n",
    "•\tFrom this information, create a variable called best_predictive_feature, which: <br>\n",
    "o\tShould be a dictionary containing the best predictive feature name as a key and the evaluation score (for the metric you chose) as the value. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2623987-c332-440f-86d2-5a6dc42056e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N       0\n",
      "P       0\n",
      "K       0\n",
      "ph      0\n",
      "crop    0\n",
      "dtype: int64\n",
      "                 N            P            K           ph\n",
      "count  2200.000000  2200.000000  2200.000000  2200.000000\n",
      "mean     50.551818    53.362727    48.149091     6.469480\n",
      "std      36.917334    32.985883    50.647931     0.773938\n",
      "min       0.000000     5.000000     5.000000     3.504752\n",
      "25%      21.000000    28.000000    20.000000     5.971693\n",
      "50%      37.000000    51.000000    32.000000     6.425045\n",
      "75%      84.250000    68.000000    49.000000     6.923643\n",
      "max     140.000000   145.000000   205.000000     9.935091\n",
      "Feature Importance:\n",
      "   Feature  Importance\n",
      "2       K    0.316785\n",
      "1       P    0.265932\n",
      "0       N    0.211928\n",
      "3      ph    0.205355\n",
      "\n",
      "Evaluating models with Feature Importance selected features:\n",
      "Best parameters for knn: {'knn__n_neighbors': 5}\n",
      "Best score for knn: 0.6392045454545455\n",
      "Best parameters for decision_tree: {'decision_tree__max_depth': 10}\n",
      "Best score for decision_tree: 0.6636363636363637\n",
      "Best parameters for logistic_regression: {'logistic_regression__C': 10.0}\n",
      "Best score for logistic_regression: 0.65625\n",
      "Best parameters for random_forest: {'random_forest__max_depth': 10, 'random_forest__n_estimators': 100}\n",
      "Best score for random_forest: 0.6556818181818181\n",
      "Best parameters for svc: {'svc__C': 1.0, 'svc__kernel': 'rbf'}\n",
      "Best score for svc: 0.6471590909090909\n",
      "Test set accuracy of the best knn model: 0.6363636363636364\n",
      "Test set accuracy of the best decision_tree model: 0.6818181818181818\n",
      "Test set accuracy of the best logistic_regression model: 0.6113636363636363\n",
      "Test set accuracy of the best random_forest model: 0.6295454545454545\n",
      "Test set accuracy of the best svc model: 0.5977272727272728\n",
      "Best Performing Model: decision_tree\n",
      "Best Crop: pigeonpeas\n",
      "RFECV Selected Features: Index(['N', 'P', 'K', 'ph'], dtype='object')\n",
      "\n",
      "Evaluating models with RFECV selected features:\n",
      "Best parameters for knn: {'knn__n_neighbors': 3}\n",
      "Best score for knn: 0.7329545454545454\n",
      "Best parameters for decision_tree: {'decision_tree__max_depth': 9}\n",
      "Best score for decision_tree: 0.7886363636363637\n",
      "Best parameters for logistic_regression: {'logistic_regression__C': 10.0}\n",
      "Best score for logistic_regression: 0.6897727272727272\n",
      "Best parameters for random_forest: {'random_forest__max_depth': 10, 'random_forest__n_estimators': 100}\n",
      "Best score for random_forest: 0.8022727272727274\n",
      "Best parameters for svc: {'svc__C': 1.0, 'svc__kernel': 'rbf'}\n",
      "Best score for svc: 0.75625\n",
      "Test set accuracy of the best knn model: 0.7727272727272727\n",
      "Test set accuracy of the best decision_tree model: 0.7931818181818182\n",
      "Test set accuracy of the best logistic_regression model: 0.6727272727272727\n",
      "Test set accuracy of the best random_forest model: 0.825\n",
      "Test set accuracy of the best svc model: 0.7659090909090909\n",
      "Best Performing Model: random_forest\n",
      "Best Crop: blackgram\n",
      "\n",
      "Ensemble Test Set Accuracy: 0.6886363636363636\n",
      "Best parameters for knn (RandomizedSearchCV): {'knn__n_neighbors': 10}\n",
      "Best score for knn (RandomizedSearchCV): 0.7392045454545454\n",
      "Best parameters for decision_tree (RandomizedSearchCV): {'decision_tree__max_depth': 10}\n",
      "Best score for decision_tree (RandomizedSearchCV): 0.7789772727272728\n",
      "Best parameters for logistic_regression (RandomizedSearchCV): {'logistic_regression__C': 100.0}\n",
      "Best score for logistic_regression (RandomizedSearchCV): 0.6948863636363637\n",
      "Best parameters for random_forest (RandomizedSearchCV): {'random_forest__n_estimators': 50, 'random_forest__max_depth': 10}\n",
      "Best score for random_forest (RandomizedSearchCV): 0.7988636363636363\n",
      "Best parameters for svc (RandomizedSearchCV): {'svc__kernel': 'rbf', 'svc__C': 10.0}\n",
      "Best score for svc (RandomizedSearchCV): 0.7823863636363637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "crops = pd.read_csv(\"soil_measures.csv\")\n",
    "\n",
    "# Check for null values and describe the dataset\n",
    "print(crops.isna().sum())\n",
    "print(crops.describe())\n",
    "\n",
    "# Encode the target variable using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(crops['crop'])\n",
    "\n",
    "# Define the features and target variable\n",
    "X = crops[['N', 'P', 'K', 'ph']]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# Initialize variables to store the best score and best model name\n",
    "best_score = 0\n",
    "best_model_name = \"\"\n",
    "\n",
    "# Define a list of classification models to be included in the pipeline\n",
    "models = [\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('decision_tree', DecisionTreeClassifier()),\n",
    "    ('logistic_regression', LogisticRegression(max_iter=10000, solver='saga')),\n",
    "    ('random_forest', RandomForestClassifier()),\n",
    "    ('svc', SVC())\n",
    "]\n",
    "\n",
    "# Define parameter grids for GridSearchCV for each model\n",
    "param_grids = {\n",
    "    'knn': {'knn__n_neighbors': [2, 3, 4, 5, 6]},\n",
    "    'decision_tree': {'decision_tree__max_depth': [5, 6, 7, 8, 9, 10]},\n",
    "    'logistic_regression': {'logistic_regression__C': [0.1, 1.0, 10.0]},\n",
    "    'random_forest': {'random_forest__n_estimators': [50, 100], 'random_forest__max_depth': [None, 10]},\n",
    "    'svc': {'svc__C': [0.1, 1.0], 'svc__kernel': ['linear', 'rbf']}\n",
    "}\n",
    "\n",
    "# Function to perform GridSearchCV and evaluate models\n",
    "def evaluate_models(X_train_selected, X_test_selected):\n",
    "    global best_score, best_model_name\n",
    "    best_pipelines = {}\n",
    "    for name, model in models:\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            (name, model)\n",
    "        ])\n",
    "        grid_search = GridSearchCV(pipeline, param_grids[name], cv=5, n_jobs=-1)\n",
    "        grid_search.fit(X_train_selected, y_train)\n",
    "        \n",
    "        print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "        print(f\"Best score for {name}: {grid_search.best_score_}\")\n",
    "        \n",
    "        best_pipelines[name] = grid_search.best_estimator_\n",
    "\n",
    "        if grid_search.best_score_ > best_score:\n",
    "            best_score = grid_search.best_score_\n",
    "            best_model_name = name\n",
    "\n",
    "    # Evaluate the best model on the test set for each pipeline\n",
    "    for name, best_pipeline in best_pipelines.items():\n",
    "        y_pred = best_pipeline.predict(X_test_selected)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Test set accuracy of the best {name} model: {accuracy}\")\n",
    "    \n",
    "    # Predict the best crop using the best performing model\n",
    "    best_pipeline = best_pipelines[best_model_name]\n",
    "    best_crop_prediction = best_pipeline.predict(X_test_selected)\n",
    "\n",
    "    # Find the most frequent predicted crop using pandas mode function\n",
    "    best_crop_name = label_encoder.inverse_transform([pd.Series(best_crop_prediction).mode()[0]])[0]\n",
    "\n",
    "    print(\"Best Performing Model:\", best_model_name)\n",
    "    print(\"Best Crop:\", best_crop_name)\n",
    "\n",
    "# Feature Importance: Select features based on their importance scores from a RandomForestClassifier.\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "importances = rf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(\"Feature Importance:\\n\", feature_importance_df)\n",
    "\n",
    "top_features_importance = feature_importance_df['Feature'].head(3).tolist()\n",
    "X_train_importance_selected = X_train[top_features_importance]\n",
    "X_test_importance_selected = X_test[top_features_importance]\n",
    "\n",
    "print(\"\\nEvaluating models with Feature Importance selected features:\")\n",
    "evaluate_models(X_train_importance_selected, X_test_importance_selected)\n",
    "\n",
    "# RFECV: Automatically select the optimal number of features using recursive feature elimination with cross-validation.\n",
    "rfecv_selector = RFECV(RandomForestClassifier(), step=1, cv=5, n_jobs=-1)\n",
    "rfecv_selector.fit(X_train, y_train)\n",
    "selected_features_rfecv = X.columns[rfecv_selector.support_]\n",
    "print(\"RFECV Selected Features:\", selected_features_rfecv)\n",
    "\n",
    "X_train_rfecv_selected = rfecv_selector.transform(X_train)\n",
    "X_test_rfecv_selected = rfecv_selector.transform(X_test)\n",
    "\n",
    "print(\"\\nEvaluating models with RFECV selected features:\")\n",
    "evaluate_models(X_train_rfecv_selected, X_test_rfecv_selected)\n",
    "\n",
    "# Ensemble Methods: Combine multiple models to improve overall performance.\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('logistic_regression', LogisticRegression(C=10.0, max_iter=20000, solver='saga')),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, max_depth=10)),\n",
    "    ('svc', SVC(C=1.0, kernel='linear'))\n",
    "], voting='hard', n_jobs=-1)\n",
    "\n",
    "ensemble.fit(X_train_rfecv_selected, y_train)\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble.predict(X_test_rfecv_selected))\n",
    "print(f\"\\nEnsemble Test Set Accuracy: {ensemble_accuracy}\")\n",
    "\n",
    "# RandomizedSearchCV: Explore a broader range of hyperparameters to find the best model configuration.\n",
    "param_grids_randomized = {\n",
    "    'knn': {'knn__n_neighbors': range(1, 20)},\n",
    "    'decision_tree': {'decision_tree__max_depth': range(1, 20)},\n",
    "    'logistic_regression': {'logistic_regression__C': [0.01, 0.1, 1.0, 10.0, 100.0]},\n",
    "    'random_forest': {'random_forest__n_estimators': [50, 100, 200], 'random_forest__max_depth': [None, 10, 20]},\n",
    "    'svc': {'svc__C': [0.1, 1.0, 10.0], 'svc__kernel': ['linear', 'rbf']}\n",
    "}\n",
    "\n",
    "best_pipelines_randomized = {}\n",
    "for name, model in models:\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        (name, model)\n",
    "    ])\n",
    "    random_search = RandomizedSearchCV(pipeline, param_grids_randomized[name], n_iter=5, cv=5, random_state=12, n_jobs=-1)\n",
    "    random_search.fit(X_train_rfecv_selected, y_train)\n",
    "    \n",
    "    print(f\"Best parameters for {name} (RandomizedSearchCV): {random_search.best_params_}\")\n",
    "    print(f\"Best score for {name} (RandomizedSearchCV): {random_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e84f42-0ed3-4efa-9fbf-4851c633de11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
